{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_list = tf.test.gpu_device_name()\n",
    "device_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Importing sklearn.metrics libraries\n",
    "\n",
    "# Libraries required for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"deceptive-opinion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>hotel</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truthful</td>\n",
       "      <td>conrad</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>We stayed for a one night getaway with family ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>truthful</td>\n",
       "      <td>hyatt</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>Triple A rate with upgrade to view room was le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truthful</td>\n",
       "      <td>hyatt</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>This comes a little late as I'm finally catchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truthful</td>\n",
       "      <td>omni</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truthful</td>\n",
       "      <td>hyatt</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>I asked for a high floor away from the elevato...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deceptive   hotel  polarity       source  \\\n",
       "0  truthful  conrad  positive  TripAdvisor   \n",
       "1  truthful   hyatt  positive  TripAdvisor   \n",
       "2  truthful   hyatt  positive  TripAdvisor   \n",
       "3  truthful    omni  positive  TripAdvisor   \n",
       "4  truthful   hyatt  positive  TripAdvisor   \n",
       "\n",
       "                                                text  \n",
       "0  We stayed for a one night getaway with family ...  \n",
       "1  Triple A rate with upgrade to view room was le...  \n",
       "2  This comes a little late as I'm finally catchi...  \n",
       "3  The Omni Chicago really delivers on all fronts...  \n",
       "4  I asked for a high floor away from the elevato...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600 entries, 0 to 1599\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   deceptive  1600 non-null   object\n",
      " 1   hotel      1600 non-null   object\n",
      " 2   polarity   1600 non-null   object\n",
      " 3   source     1600 non-null   object\n",
      " 4   text       1600 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['hotel', 'source'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deceptive\n",
       "truthful     800\n",
       "deceptive    800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.deceptive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"deceptive\"] == \"truthful\", \"LABEL\"] = 1\n",
    "data.loc[data[\"deceptive\"] == \"deceptive\", \"LABEL\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>We stayed for a one night getaway with family ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>Triple A rate with upgrade to view room was le...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>This comes a little late as I'm finally catchi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>I asked for a high floor away from the elevato...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deceptive  polarity                                               text  \\\n",
       "0  truthful  positive  We stayed for a one night getaway with family ...   \n",
       "1  truthful  positive  Triple A rate with upgrade to view room was le...   \n",
       "2  truthful  positive  This comes a little late as I'm finally catchi...   \n",
       "3  truthful  positive  The Omni Chicago really delivers on all fronts...   \n",
       "4  truthful  positive  I asked for a high floor away from the elevato...   \n",
       "\n",
       "   LABEL  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "2    1.0  \n",
       "3    1.0  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>negative</td>\n",
       "      <td>Problems started when I booked the InterContin...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>negative</td>\n",
       "      <td>The Amalfi Hotel has a beautiful website and i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>negative</td>\n",
       "      <td>The Intercontinental Chicago Magnificent Mile ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>negative</td>\n",
       "      <td>The Palmer House Hilton, while it looks good i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>negative</td>\n",
       "      <td>As a former Chicagoan, I'm appalled at the Ama...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      deceptive  polarity                                               text  \\\n",
       "1595  deceptive  negative  Problems started when I booked the InterContin...   \n",
       "1596  deceptive  negative  The Amalfi Hotel has a beautiful website and i...   \n",
       "1597  deceptive  negative  The Intercontinental Chicago Magnificent Mile ...   \n",
       "1598  deceptive  negative  The Palmer House Hilton, while it looks good i...   \n",
       "1599  deceptive  negative  As a former Chicagoan, I'm appalled at the Ama...   \n",
       "\n",
       "      LABEL  \n",
       "1595    0.0  \n",
       "1596    0.0  \n",
       "1597    0.0  \n",
       "1598    0.0  \n",
       "1599    0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "positive    800\n",
       "negative    800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_length'] = data['text'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>We stayed for a one night getaway with family ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>Triple A rate with upgrade to view room was le...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>This comes a little late as I'm finally catchi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truthful</td>\n",
       "      <td>positive</td>\n",
       "      <td>I asked for a high floor away from the elevato...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deceptive  polarity                                               text  \\\n",
       "0  truthful  positive  We stayed for a one night getaway with family ...   \n",
       "1  truthful  positive  Triple A rate with upgrade to view room was le...   \n",
       "2  truthful  positive  This comes a little late as I'm finally catchi...   \n",
       "3  truthful  positive  The Omni Chicago really delivers on all fronts...   \n",
       "4  truthful  positive  I asked for a high floor away from the elevato...   \n",
       "\n",
       "   LABEL  review_length  \n",
       "0    1.0            572  \n",
       "1    1.0            286  \n",
       "2    1.0           1104  \n",
       "3    1.0            707  \n",
       "4    1.0            384  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4159"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(data['review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data['text'] + ' ' + data['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.DataFrame(data[[\"combined\", \"LABEL\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LABEL'] = data['LABEL'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined     object\n",
      "LABEL       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_subset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df_subset.combined.values\n",
    "LABEL = df_subset.LABEL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    # remove url and hashtag\n",
    "    for i in range(data.shape[0]):\n",
    "        text = data[i].lower()\n",
    "        text1 = ''.join([word+\" \" for word in text.split()])\n",
    "        data[i] = text1\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "                       '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    space_pattern = '\\s+'\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        text_string = data[i]\n",
    "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
    "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "        parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "        # remove punctuation\n",
    "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n",
    "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
    "        data[i] = parsed_text\n",
    "    return data\n",
    "\n",
    "\n",
    "combined = preprocess(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['we stayed for a one night getaway with family on a thursday triple aaa rate of 173 was a steal 7th floor room complete with 44in plasma tv bose stereo voss and evian water and gorgeous bathroomno tub but was fine for us concierge was very helpful you cannot beat this location only flaw was breakfast was pricey and service was very very slow2hours for four kids and four adults on a friday morning even though there were only two other tables in the restaurant food was very good so it was worth the wait i would return in a heartbeat a gem in chicago positive ',\n",
       "       'triple a rate with upgrade to view room was less than 200 which also included breakfast vouchers had a great view of river lake wrigley bldg tribune bldg most major restaurants shopping sightseeing attractions within walking distance large room with a very comfortable bed positive ',\n",
       "       'this comes a little late as im finally catching up on my reviews from the past several months a dear friend and i stayed at the hyatt regency in late october 2007 for one night while visiting a friend and her husband from out of town this hotel is perfect imo easy check in and check out lovely clean comfortable rooms with great views of the city i know this area pretty well and its very convenient to many downtown chicago attractions we had dinner and went clubing with our friends around division st we had no problems getting cabs back and forth to the hyatt and theres even public transportation right near by but we didnt bother since we only needed cabs from and to the hotel parking as is usual for chicago was expensive but we were able to get our car out quickly however we left on a sunday morning not exactly a high traffic time although it was a bears homegame day so a bit busier than usual i would think no problems at all and the best part is that we got a rate of 100 through hotwire a downright steal for this area of chicago and the quality of the hotel positive ',\n",
       "       ...,\n",
       "       'the intercontinental chicago magnificent mile the outside of the hotel itself is as the name says is pretty magnificent despite being set in what has to be the filthiest section in the city for the cost of the rooms starting at 17900 a night you would think they would have a competent parking attendant i was delayed from a very important client due to a latency issue with my reserved room and if that was not enough to top it off the room service had the nerve to bring up room temperature pasta and a bottle of champagne that had the seal previously broken free of the cork needless to say this will be the last time this hotel ever is to grace so much as another dollar bill from my account i would highly recommend another hotel with better accommodations negative ',\n",
       "       'the palmer house hilton while it looks good in pictures and the outside is actually a disaster of a hotel when i went through the lobby was dirty my room hadnt been cleaned and smelled thoroughly of smoke when i requested more pillows the lady on the phone scoffed at me and said shed send them up it took over an hour for 2 pillows this hotel is a good example that what you pay for isnt always what you get i will not be returning negative ',\n",
       "       'as a former chicagoan im appalled at the amalfi hotel chicago first of all i was expecting luxury and hospitality neither of which i received theres an experience designer who is supposed to be like a personal concierge but my experience with my ed was terrible i felt like he was trying to pressure me into staying more days than i wanted to not only that but i couldnt understand what he was saying most of the time because he was talking so fast when i finally got to my room i was disappointed with the quality of the furniture and the rooms cleanliness i had to ask for a maid to come and give me clean towels because some of the towels in the bathroom were damp on top of that the bed was messily done i could have done a better job on my own bed at home i was angry at this point because i was paying a lot of money for every night i was staying at amalfi and i didnt expect to be greeted with wet towels i needed to use the wifi to download some important documents and the internet was surprisingly slow even a very basic hotel or motel could have offered better maybe even faster internet access when i finally checked out of the amalfi i made sure that my supposed personal concierge knew all of the problems id had with my room and the hotel i was glad to see the amalfi getting smaller in the mirror as i drove away negative '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  we stayed for a one night getaway with family on a thursday triple aaa rate of 173 was a steal 7th floor room complete with 44in plasma tv bose stereo voss and evian water and gorgeous bathroomno tub but was fine for us concierge was very helpful you cannot beat this location only flaw was breakfast was pricey and service was very very slow2hours for four kids and four adults on a friday morning even though there were only two other tables in the restaurant food was very good so it was worth the wait i would return in a heartbeat a gem in chicago positive \n",
      "Tokenized:  ['we', 'stayed', 'for', 'a', 'one', 'night', 'get', '##away', 'with', 'family', 'on', 'a', 'thursday', 'triple', 'aaa', 'rate', 'of', '173', 'was', 'a', 'steal', '7th', 'floor', 'room', 'complete', 'with', '44', '##in', 'plasma', 'tv', 'bose', 'stereo', 'voss', 'and', 'ev', '##ian', 'water', 'and', 'gorgeous', 'bathroom', '##no', 'tub', 'but', 'was', 'fine', 'for', 'us', 'con', '##cier', '##ge', 'was', 'very', 'helpful', 'you', 'cannot', 'beat', 'this', 'location', 'only', 'flaw', 'was', 'breakfast', 'was', 'price', '##y', 'and', 'service', 'was', 'very', 'very', 'slow', '##2', '##ho', '##urs', 'for', 'four', 'kids', 'and', 'four', 'adults', 'on', 'a', 'friday', 'morning', 'even', 'though', 'there', 'were', 'only', 'two', 'other', 'tables', 'in', 'the', 'restaurant', 'food', 'was', 'very', 'good', 'so', 'it', 'was', 'worth', 'the', 'wait', 'i', 'would', 'return', 'in', 'a', 'heartbeat', 'a', 'gem', 'in', 'chicago', 'positive']\n",
      "Token IDs:  [2057, 4370, 2005, 1037, 2028, 2305, 2131, 9497, 2007, 2155, 2006, 1037, 9432, 6420, 13360, 3446, 1997, 19410, 2001, 1037, 8954, 5504, 2723, 2282, 3143, 2007, 4008, 2378, 12123, 2694, 21299, 12991, 24878, 1998, 23408, 2937, 2300, 1998, 9882, 5723, 3630, 14366, 2021, 2001, 2986, 2005, 2149, 9530, 19562, 3351, 2001, 2200, 14044, 2017, 3685, 3786, 2023, 3295, 2069, 28450, 2001, 6350, 2001, 3976, 2100, 1998, 2326, 2001, 2200, 2200, 4030, 2475, 6806, 9236, 2005, 2176, 4268, 1998, 2176, 6001, 2006, 1037, 5958, 2851, 2130, 2295, 2045, 2020, 2069, 2048, 2060, 7251, 1999, 1996, 4825, 2833, 2001, 2200, 2204, 2061, 2009, 2001, 4276, 1996, 3524, 1045, 2052, 2709, 1999, 1037, 12251, 1037, 17070, 1999, 3190, 3893]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', combined[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(\n",
    "    tokenizer.tokenize(combined[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  we stayed for a one night getaway with family on a thursday triple aaa rate of 173 was a steal 7th floor room complete with 44in plasma tv bose stereo voss and evian water and gorgeous bathroomno tub but was fine for us concierge was very helpful you cannot beat this location only flaw was breakfast was pricey and service was very very slow2hours for four kids and four adults on a friday morning even though there were only two other tables in the restaurant food was very good so it was worth the wait i would return in a heartbeat a gem in chicago positive \n",
      "Token IDs: tensor([  101,  2057,  4370,  2005,  1037,  2028,  2305,  2131,  9497,  2007,\n",
      "         2155,  2006,  1037,  9432,  6420, 13360,  3446,  1997, 19410,  2001,\n",
      "         1037,  8954,  5504,  2723,  2282,  3143,  2007,  4008,  2378, 12123,\n",
      "         2694, 21299, 12991, 24878,  1998, 23408,  2937,  2300,  1998,  9882,\n",
      "         5723,  3630, 14366,  2021,  2001,  2986,  2005,  2149,  9530, 19562,\n",
      "         3351,  2001,  2200, 14044,  2017,  3685,  3786,  2023,  3295,  2069,\n",
      "        28450,  2001,  6350,  2001,  3976,  2100,  1998,  2326,  2001,  2200,\n",
      "         2200,  4030,  2475,  6806,  9236,  2005,  2176,  4268,  1998,  2176,\n",
      "         6001,  2006,  1037,  5958,  2851,  2130,  2295,  2045,  2020,  2069,\n",
      "         2048,  2060,  7251,  1999,  1996,  4825,  2833,  2001,  2200,  2204,\n",
      "         2061,  2009,  2001,  4276,  1996,  3524,  1045,  2052,  2709,  1999,\n",
      "         1037, 12251,  1037, 17070,  1999,  3190,  3893,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import time\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for tweet in combined:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        tweet,                      # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=512,           # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,   # Construct attn. masks.\n",
    "        return_tensors='pt',     # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(LABEL)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', combined[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,440 training samples\n",
      "  160 validation samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    import datetime\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, embed_dim=768, hidden_dim=128, num_layers=2, num_labels=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.softmax = nn.functional.softmax\n",
    "        self.bert = bert_model  # Assign the BertModel directly\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(\n",
    "            hidden_dim * (2 if bidirectional else 1), num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                           token_type_ids=token_type_ids)[0]  # Get the BERT output\n",
    "        lstm_out, _ = self.lstm(output)\n",
    "\n",
    "        # Use the last hidden state of the LSTM as input to the classifier\n",
    "        if self.bidirectional:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        output = self.dropout(lstm_out)\n",
    "        logits = self.classifier(output)\n",
    "        return self.softmax(logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pass the loaded BERT model to your classifier\n",
    "model_bilstm = BertBiLSTMClassifier(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set parameters\n",
    "epochs = 4\n",
    "learning_rate = 5e-5\n",
    "optimizer = optim.AdamW(model_bilstm.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "best_accuracy = 0\n",
    "\n",
    "device = torch.device(\"cpu\")  # Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3\n",
      "Training...\n",
      "  Accuracy: 0.6972222222222222\n",
      "  Training loss: 0.037298186019890836\n",
      "\n",
      "Validation...\n",
      "  Accuracy: 0.8625\n",
      "  Validation loss: 0.029519927129149436\n",
      "  This epoch took: 3:20:16\n",
      "\n",
      "  roc_auc score:  0.8510101010101011\n",
      "  F1 score: 0.828125\n",
      "\n",
      "Epoch 2 / 3\n",
      "Training...\n",
      "  Accuracy: 0.8527777777777777\n",
      "  Training loss: 0.02921379158894221\n",
      "\n",
      "Validation...\n",
      "  Accuracy: 0.9275\n",
      "  Validation loss: 0.029978754557669164\n",
      "  This epoch took: 0:57:37\n",
      "\n",
      "  roc_auc score:  0.8207070707070706\n",
      "  F1 score: 0.7833333333333333\n",
      "\n",
      "Epoch 3 / 3\n",
      "Training...\n",
      "  Accuracy: 0.9090277777777778\n",
      "  Training loss: 0.025512815680768755\n",
      "\n",
      "Validation...\n",
      "  Accuracy: 0.9445\n",
      "  Validation loss: 0.029277219623327255\n",
      "  This epoch took: 0:53:25\n",
      "\n",
      "  roc_auc score:  0.9409090909090909\n",
      "  F1 score: 0.9481818181818182\n",
      "===\n",
      "Summary\n",
      "Total time 5:11:18 (h:mm:ss)\n",
      "best acc: 0.9481\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# Assuming you have defined your model, criterion, optimizer, and data loaders\n",
    "# model_bilstm, criterion, optimizer, train_dataloader, validation_dataloader\n",
    "\n",
    "# Initialize variables for tracking best accuracy\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Set the number of epochs\n",
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Training\n",
    "    print(\"\")\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model_bilstm.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        # Ensure labels are of type torch.long\n",
    "        labels = batch[2].to(device).long()\n",
    "\n",
    "        model_bilstm.zero_grad()\n",
    "        out = model_bilstm(input_ids=input_ids,\n",
    "                           attention_mask=input_mask, token_type_ids=None)\n",
    "        loss = criterion(out, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_bilstm.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        total_train_accuracy += torch.sum(pred == labels).item()\n",
    "\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
    "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    print(\"\")\n",
    "    print(\"Validation...\")\n",
    "    model_bilstm.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        # Ensure labels are of type torch.long\n",
    "        labels = batch[2].to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model_bilstm(input_ids=input_ids,\n",
    "                               attention_mask=input_mask, token_type_ids=None)\n",
    "            loss = criterion(out, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            total_eval_accuracy += torch.sum(pred == labels).item()\n",
    "\n",
    "            y_true.append(labels.flatten())\n",
    "            y_pred.append(pred.flatten())\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
    "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  This epoch took: {:}\".format(training_time))\n",
    "    print()\n",
    "\n",
    "    y_true = torch.cat(y_true).tolist()\n",
    "    y_pred = torch.cat(y_pred).tolist()\n",
    "    print('  roc_auc score: ', roc_auc_score(y_true, y_pred))\n",
    "    print('  F1 score:', f1_score(y_true, y_pred))\n",
    "\n",
    "    training_stats.append({\n",
    "        'epoch': epoch_i + 1,\n",
    "        'Train Accur.': avg_train_accuracy,\n",
    "        'Training Loss': avg_train_loss,\n",
    "        'Valid. Loss': avg_val_loss,\n",
    "        'Valid. Accur.': avg_val_accuracy,\n",
    "        'Training Time': training_time,\n",
    "    })\n",
    "\n",
    "    if avg_val_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_val_accuracy\n",
    "        best_model = model_bilstm\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Summary\")\n",
    "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "print('best acc:', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BiLSTM = \"bilstm_model.pt\"\n",
    "torch.save(best_model, PATH_BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, embed_dim=768, hidden_dim=128, num_layers=2, num_labels=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.softmax = nn.functional.softmax\n",
    "        self.bert = bert_model  # Assign the BertModel directly\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(\n",
    "            hidden_dim * (2 if bidirectional else 1), num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                           token_type_ids=token_type_ids)[0]  # Get the BERT output\n",
    "        lstm_out, _ = self.lstm(output)\n",
    "\n",
    "        # Use the last hidden state of the LSTM as input to the classifier\n",
    "        if self.bidirectional:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        output = self.dropout(lstm_out)\n",
    "        logits = self.classifier(output)\n",
    "        return self.softmax(logits, 1)\n",
    "\n",
    "\n",
    "def checkbilstm(text_input):\n",
    "    # Load the saved model\n",
    "    PATH_BiLSTM = \"bilstm_model.pt\"\n",
    "    model = torch.load(PATH_BiLSTM)\n",
    "\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Create an instance of your custom classifier\n",
    "    model_bilstm = BertBiLSTMClassifier(bert_model)\n",
    "\n",
    "    # Load the saved model weights into your custom classifier\n",
    "    model_bilstm.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model_bilstm.eval()\n",
    "\n",
    "    # Define a function to preprocess text input\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize the text and convert it into input IDs, attention mask, and token type IDs\n",
    "        inputs = tokenizer(text, return_tensors='pt',\n",
    "                           max_length=512, truncation=True, padding=True)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "    # Define a function to predict\n",
    "\n",
    "    def predict_single_text(text):\n",
    "        # Preprocess the text input\n",
    "        input_ids, attention_mask, token_type_ids = preprocess_text(text)\n",
    "\n",
    "        # Perform the prediction\n",
    "        with torch.no_grad():\n",
    "            output = model_bilstm(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        predicted_labels = torch.argmax(output, dim=1)\n",
    "        return predicted_labels.item()\n",
    "\n",
    "    # Perform prediction\n",
    "    predicted_label = predict_single_text(text_input)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "v = \"my german shepherd absolutely loves this my stuff has been officially tooth markfree for the first time in a very long time good no pet products\"\n",
    "x = checkbilstm(v)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
